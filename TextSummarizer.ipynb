{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71MblvPMWyDI",
        "outputId": "44f5c758-d396-4604-cff3-ed49b1213a38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bard\n",
            "  Downloading bard-0.1-py3-none-any.whl.metadata (183 bytes)\n",
            "Collecting api\n",
            "  Downloading api-0.0.7.tar.gz (2.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from api) (2.32.3)\n",
            "Collecting nose (from api)\n",
            "  Downloading nose-1.3.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->api) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->api) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->api) (2025.1.31)\n",
            "Downloading bard-0.1-py3-none-any.whl (978 bytes)\n",
            "Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: api\n",
            "  Building wheel for api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for api: filename=api-0.0.7-py3-none-any.whl size=2305 sha256=aea4494faab31dbb50dec3944688f479927ab5b47846b0c410cb6d29ba8d7de2\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/a8/f7/3a8110a27e68f3f5611af9f9203d1f6436a52f67f363053a58\n",
            "Successfully built api\n",
            "Installing collected packages: nose, bard, api\n",
            "Successfully installed api-0.0.7 bard-0.1 nose-1.3.7\n"
          ]
        }
      ],
      "source": [
        "!pip install bardapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AbGPC06zXDpb"
      },
      "outputs": [],
      "source": [
        "from bardapi import Bard\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yhv-U-LsYCMG"
      },
      "outputs": [],
      "source": [
        "os.environ['_BARD_API_KEY']=\"Your_API_key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6V4s2lJZV3X",
        "outputId": "24df26eb-fe7a-4296-e2db-f7bcd4a64a98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The 2023 Ashes series between England and Australia has concluded. \n",
            "\n",
            "**Here's a summary of the series:**\n",
            "\n",
            "*   **Dates:** June 16 - July 31, 2023\n",
            "*   **Location:** England\n",
            "*   **Result:** The five-match series was drawn 2-2. Australia retained the Ashes.\n",
            "\n",
            "**Key Points:**\n",
            "\n",
            "*   The series was part of the 2023-2025 ICC World Test Championship.\n",
            "*   The venues for the matches were Edgbaston, Lord's, Headingley, Old Trafford, and The Oval.\n",
            "*   The series was notable for its close contests and exciting moments.\n",
            "\n",
            "You can find more detailed information about the series, including scores, statistics, and news, on websites like Cricbuzz and Wikipedia.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Set your input text\n",
        "# input_text = \"Why is the sky blue?\"\n",
        "input_text = \"What is the current status of Ashes Series 2023\"\n",
        "print(Bard().get_answer(input_text)['content'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNAHqeiFerjd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6lKVY-qeneu",
        "outputId": "19626e3b-6133-4d1d-bd49-d8b4c8e98879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.3.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Downloading pypdf-5.3.0-py3-none-any.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6NM8MpJ0e0zo"
      },
      "outputs": [],
      "source": [
        "from pypdf import PdfReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD0Qaao_e7eE",
        "outputId": "a1f1fe16-c240-42ce-a43f-8991341296a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rsJXP0a4fWWO"
      },
      "outputs": [],
      "source": [
        "directory = '/content/drive/MyDrive/'\n",
        "filename  = 'distributed-and-cloud-computing.pdf'\n",
        "# filename  = 'Art-of-War.pdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZrOXtVbpfvfW"
      },
      "outputs": [],
      "source": [
        "# creating a pdf file object\n",
        "pdfFileObject = open(directory+filename, 'rb')\n",
        "# creating a pdf reader object\n",
        "pdfReader = PdfReader(pdfFileObject)\n",
        "text=[]\n",
        "summary=' '\n",
        "#Storing the pages in a list\n",
        "for i in range(0,len(pdfReader.pages)):\n",
        "  # creating a page object\n",
        "  pageObj = pdfReader.pages[i].extract_text()\n",
        "  pageObj= pageObj.replace('\\t\\r','')\n",
        "  pageObj= pageObj.replace('\\xa0','')\n",
        "  # extracting text from page\n",
        "  text.append(pageObj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHprtj4pf8sR",
        "outputId": "d73f6d0f-a107-42da-82cf-37ec7ae0158e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Pages =  671\n",
            "Compressed Pages =  224\n"
          ]
        }
      ],
      "source": [
        "# Merge multiple page - to reduce API Calls\n",
        "def join_elements(lst, chars_per_element):\n",
        "    new_lst = []\n",
        "    for i in range(0, len(lst), chars_per_element):\n",
        "        new_lst.append(''.join(lst[i:i+chars_per_element]))\n",
        "    return new_lst\n",
        "\n",
        "# Option to keep x elements per list element\n",
        "new_text = join_elements(text, 3)\n",
        "\n",
        "print(f\"Original Pages = \",len(text))\n",
        "print(f\"Compressed Pages = \",len(new_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gRkCqICagCB2"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt):\n",
        "  response = Bard().get_answer(prompt)['content']\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFaH51QrgHyp",
        "outputId": "b3aaac00-ece3-46a0-a201-6f13b33e3d17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Understood. I am ready to summarize the provided text within the 100-word limit. Please provide the text.\n",
            "\n",
            "Distributed computing involves multiple interconnected computers working together as a single system.  Cloud computing leverages this by providing on-demand access to shared computing resources (servers, storage, applications) over a network, typically the internet.  Cloud services are often categorized as Infrastructure as a Service (IaaS), Platform as a Service (PaaS), or Software as a Service (SaaS).  Key characteristics include scalability, elasticity, and pay-as-you-go pricing.  Distributed systems concepts like concurrency, fault tolerance, and consistency are crucial in designing robust cloud applications.  Cloud computing simplifies IT management and enables access to powerful resources, but also introduces challenges related to security, privacy, and vendor lock-in.\n",
            "\n",
            "This page intentionally left blank.\n",
            "\n",
            "This book, \"Distributed and Cloud Computing: From Parallel Processing to the Internet of Things,\" by Kai Hwang, Geoffrey C. Fox, and Jack J. Dongarra, explores the evolution of computing from parallel processing to modern distributed systems and cloud computing, culminating in the Internet of Things.  Published by Morgan Kaufmann, it covers fundamental concepts and emerging technologies in these interconnected fields.\n",
            "\n",
            "This page provides copyright and publication information for a book published by Morgan Kaufmann, an imprint of Elsevier.  It lists trademarks, copyright notices, and disclaimers regarding the content's accuracy and liability.  It also includes standard publishing details like ISBN, Library of Congress and British Library cataloging information, and website addresses.  The book's printing location and number are also noted.  Essentially, this page is a formal statement of ownership and legal protections.\n",
            "\n",
            "This book is dedicated to the authors' wives, Jennifer, Judy, and Sue, and their many children: Tony, Andrew, Katherine, Annie, Alexis, James, Heather, Pamela, Oliver, Nick, Ben, and Katie.  The dedication is signed with the initials of the authors: KH, GF, and JD.\n",
            "\n",
            "The provided text, \"This page intentionally left blank,\" indicates a deliberate omission of content. This phrase is commonly used in printed materials to acknowledge an empty page, preventing confusion or suspicion of missing information. It serves as a simple, direct statement of a planned blank space.\n",
            "\n",
            "This chapter introduces distributed system models and enabling technologies. It covers scalable computing trends like the Internet of Things and cyber-physical systems, along with technologies such as multicore CPUs, GPUs, virtualization, and data center virtualization for cloud computing.  Different system models are explored, including clusters, grids, peer-to-peer networks, and cloud computing.  Software environments like Service-Oriented Architecture and distributed operating systems are discussed, along with parallel and distributed programming models.  Finally, the chapter examines performance metrics, scalability analysis, fault tolerance, security threats, data integrity, and energy efficiency in distributed computing.\n",
            "\n",
            "Chapter 2 explores computer clusters for parallel computing, covering cluster development trends, design objectives, and fundamental issues.  It analyzes top supercomputers, comparing cluster and MPP architectures, node architectures, and interconnects.  The chapter details cluster design principles, including single-system image features, high availability, and fault tolerance.  Job and resource management, scheduling methods, and management systems are discussed, featuring LSF and MOSIX.  Case studies of top supercomputers like Tianhe-1A, Cray XT5 Jaguar, and IBM Roadrunner are presented. Chapter 3 introduces virtual machines and virtualization of clusters and data centers, examining implementation levels, VMM design, OS-level virtualization, and middleware support. It covers virtualization structures, tools, mechanisms, hypervisors, and binary translation.\n",
            "\n",
            "This section covers virtualization, virtual clusters, and data center automation.  Virtualization encompasses CPU, memory, and I/O devices, often with hardware support.  Virtual clusters combine physical and virtual machines, enabling live migration.  Data center automation leverages virtualization for server consolidation, storage management, and cloud operating systems.  The text then transitions to cloud computing, exploring service models (IaaS, PaaS, SaaS), cloud types (public, private, hybrid), and enabling technologies.  It discusses data center design, interconnection networks, and modular data centers. Finally, it outlines cloud architecture, including a generic design, layered development, disaster recovery, and design challenges.\n",
            "\n",
            "This section discusses public cloud platforms like Google App Engine (GAE), Amazon Web Services (AWS), and Microsoft Azure, outlining their service offerings.  It explores inter-cloud resource management, including extended cloud services, resource provisioning, virtual machine management, and global resource exchange.  Cloud security and trust management are also covered, focusing on defense strategies, intrusion detection, data protection, and data center reputation.  The text then transitions to service-oriented architectures (SOA), covering REST, web services, enterprise architecture, and grid services. Message-oriented middleware, including enterprise buses and publish-subscribe models, is explained. Portals and science gateways like HUBzero are discussed, along with discovery, registries, metadata, and databases, including UDDI, semantic web, and job execution environments.\n",
            "\n",
            "This section covers workflow in service-oriented architectures, detailing basic concepts, standards, architectures, execution engines, and scripting with Swift. It then transitions to cloud programming and software environments, comparing cloud and grid platforms. It explores parallel and distributed programming paradigms, including MapReduce, Hadoop, Dryad, Sawzall, and Pig Latin. Specific programming support for Google App Engine, Amazon AWS, and Microsoft Azure is reviewed, covering file systems, databases, and services. Emerging cloud environments like Eucalyptus, OpenNebula, and Aneka are also discussed.\n",
            "\n",
            "Chapter 7 explores grid computing, covering its architecture, service modeling, and history, including CPU scavenging and OGSA.  It examines grid projects like TeraGrid, DataGrid, and ChinaGrid, focusing on resource management, brokering, and job scheduling.  The chapter discusses grid middleware, including Globus Toolkit, and application trends, performance prediction, and security measures like GSI. Chapter 8 introduces peer-to-peer (P2P) computing, outlining its basic concepts, challenges, and network system taxonomy.  It then delves into P2P overlay networks, particularly unstructured ones, setting the stage for further exploration of this distributed computing paradigm.\n",
            "\n",
            "This text covers peer-to-peer (P2P) networks and ubiquitous cloud/IoT computing.  P2P networks utilize distributed hash tables and structured overlays for routing and fault tolerance.  Trust and reputation systems are crucial for security, especially against DDoS attacks.  File sharing applications face challenges regarding copyright protection.  The text then shifts to cloud computing's role in supporting ubiquitous computing, highlighting large-scale private clouds and cloudlets.  Performance metrics and QoS in cloud computing are discussed, including benchmarking various systems.  Finally, the Internet of Things (IoT) is introduced, focusing on enabling technologies like RFID, sensor networks, and GPS, along with innovative applications.\n",
            "\n",
            "This section covers various technological advancements. It discusses the role of retailing and supply-chain management, emphasizing efficiency and optimization.  Smart power grids and smart buildings are introduced, highlighting their contributions to energy conservation and sustainability.  The concept of Cyber-Physical Systems (CPS) is explained, focusing on the integration of physical and computational components.  The chapter then explores online social and professional networking, detailing the characteristics of these platforms and applying graph-theoretic analysis to understand their structure.  It examines communities within social networks and their applications, using Facebook and Twitter as prominent examples. Finally, the section includes bibliographic notes, acknowledgements, references, homework problems, and an index.\n",
            "\n",
            "This book explores the evolution of parallel and distributed computing, integrating theory with practical design, programming, and applications.  It covers hardware, software, architectures, and programming paradigms, emphasizing performance and energy efficiency in creating high-performance clusters, networks, data centers, and cloud/grid systems.  The text aims to transform traditional multiprocessing into web-scale systems for the future internet, including social networks and the Internet of Things.  It examines contributions from leading researchers, covering distributed models, design principles, and innovative applications.  The book is divided into three parts: system models and enabling technologies; data center design, cloud platforms, and distributed programming; and computational grids, P2P networks, ubiquitous clouds, IoT, and social networks.  Cloud computing, including major public cloud platforms, is a central theme, highlighting its importance in web services and internet applications.\n",
            "\n",
            "This book covers modern distributed computing technologies, emphasizing ubiquity, agility, efficiency, scalability, availability, and programmability. It details hardware, networks, and system architecture advancements, including multi/many-core CPUs/GPUs, virtual machines/clusters, top-500 architectures, cloud platforms (Google, Amazon, Microsoft, IBM), and various grid and peer-to-peer systems.  The book also explores recent paradigm shifts, programming languages, software, and ecosystems like MapReduce, Hadoop, cloud service models, virtualization software, and service-oriented architecture.  Over 100 examples and case studies from leading vendors (Amazon, Google, Microsoft, etc.) are included.  It's designed for distributed systems courses and as a reference for professionals, with chapter sequencing suitable for one or two semester courses.\n",
            "\n",
            "This book, collaboratively written and edited over four years (2007-2011) by three lead authors, acknowledges contributions from numerous scientists, researchers, instructors, and doctoral students from ten top universities across the US, China, and Australia.  The authors express gratitude to these individuals for their dedicated work and valuable contributions throughout the writing and revision process, as well as to anonymous reviewers.  Specific contributors from institutions like Sydney University, Tsinghua University, Indiana University, and others are listed.  The book also acknowledges permissions granted for copyrighted illustrations, thanking individuals like Bill Dally and Rajkumar Buyya.  The authors appreciate Ian Foster for the Foreword, Todd Green for sponsorship, Robyn Day for editorial work, and Dennis Troutman for production.  They hope readers enjoy the book and provide feedback.\n",
            "\n",
            "This page intentionally left blank.\n",
            "\n",
            "Kai Hwang, a USC professor and Tsinghua visiting chair, specializes in computer architecture, parallel processing, and distributed systems.  He's authored eight books and over 220 papers, receiving numerous awards, including IEEE Fellow and IPDPS Founders' Award. Geoffrey Fox, Indiana University professor and associate dean, focuses on parallel architecture, distributed programming, and grid computing.  He's widely published and mentored over 60 PhD students. Jack Dongarra, University of Tennessee professor and Oak Ridge researcher, is a high-performance computing pioneer.  An ACM/IEEE/SIAM/AAAS Fellow and National Academy of Engineering member, he leads the Top-500 supercomputer benchmark.\n",
            "\n",
            "No summary needed. The provided text is a blank page.\n",
            "\n",
            "Parallel computing, like Feynman's 1940s work, tackles complex problems by dividing them into smaller, simultaneous tasks.  While individual processor speeds are limited, parallelism is achieved through techniques like pipelining and multithreading.  Modern systems, from desktops to supercomputers, utilize multiple processors.  Communication speeds have drastically increased since Feynman's time, enabling data-intensive applications.  This allows for centralized computing in massive data centers and supercomputers, serving numerous users and supporting complex simulations and data processing for various applications, from weather forecasting to stock market predictions.\n",
            "\n",
            "Distributed computing is essential to modern computer science education.  Complex systems today are data centers, not individual microprocessors, and computer science graduates must understand their construction.  Hwang, Fox, and Dongarra's text covers hardware/software architectures, cloud and distributed computing, and advanced topics like grid, peer-to-peer, and the Internet of Things, taking a systems approach with real-world examples.  The book also highlights challenges like energy consumption and the \"Internet of Things.\"  The author hopes this book encourages more distributed computing education, not as an optional topic, but as a core curriculum element.\n",
            "\n",
            "This section introduces systems modeling, clustering, and virtualization as key enabling technologies for distributed and cloud computing. Chapter 1 models distributed systems and cloud platforms, covering computer clusters, computing grids, P2P networks, and cloud computing.  It analyzes the evolution of computing and IT trends, comparing HPC and HTC systems based on architecture, OS, algorithms, protocols, security, and service models, emphasizing scalability, performance, availability, security, and energy efficiency. Chapter 2 focuses on computer clusters for scalable parallel computing, built with physical servers or virtual machines. It explores design principles, hardware, software, middleware support, scalability, availability, programmability, single system image, job management, and fault tolerance, citing examples like Tianhe-1A, Cray XT5 Jaguar, and IBM RoadRunner.\n",
            "\n",
            "Virtualization enables efficient hardware resource sharing through virtual machines (VMs), enhancing system applications and performance. This chapter explores VMs, their live migration, virtual cluster construction, resource provisioning, and virtual configuration adaptation. It emphasizes the use of virtual clusters and virtualized resource management for building dynamic grids and cloud platforms, particularly for cloud computing applications. The technology facilitates the creation of virtualized data centers and addresses the growing interest in installing VMs. The chapter also acknowledges the contributions of various authors and technical assistants in its development.\n",
            "\n",
            "This chapter introduces distributed system models and enabling technologies, focusing on scalable computing over the internet. It explores trends like the Internet of Things and cyber-physical systems. Key technologies discussed include multicore CPUs, GPU computing, virtualization, and data center virtualization for cloud computing. System models like clusters, grids, peer-to-peer networks, and cloud computing are examined. Software environments, including service-oriented architecture and distributed operating systems, are covered. The chapter concludes with considerations of performance, security, energy efficiency, and provides bibliographic notes and homework problems.\n",
            "\n",
            "Parallel and distributed computing has evolved significantly over the past 30 years, driven by applications with variable workloads and massive datasets.  This evolution spans various systems, from computer clusters and service-oriented architectures to computational grids, peer-to-peer networks, Internet clouds, and the Internet of Things.  These systems differ in hardware, OS platforms, algorithms, communication protocols, and service models.  The rise of Internet computing necessitates high-throughput computing (HTC) systems due to the increasing number of users, rendering traditional benchmarks like Linpack less relevant.  This shift requires upgrading data centers with faster servers, storage, and high-bandwidth networks.  Computer technology has progressed through five generations, from mainframes to minicomputers, personal computers, portable devices, and now HPC and HTC systems.\n",
            "\n",
            "Computing systems have evolved, shifting from centralized supercomputers to distributed clusters, grids, and cloud platforms. High-Performance Computing (HPC) focuses on raw speed, progressing from Gflops to Pflops, driven by scientific and industrial demands. High-Throughput Computing (HTC) emphasizes distributed resources, utilizing peer-to-peer (P2P) networks, cloud computing, and web services. Clusters, collections of homogeneous nodes, and P2P networks, globally distributed clients, are foundational. These technologies have facilitated the development of computational and data grids, leveraging shared web resources and massive datasets over the Internet.\n",
            "\n",
            "High-performance computing (HPC) focuses on maximizing processing speed, while high-throughput computing (HTC) prioritizes the number of tasks completed per unit of time, crucial for internet searches and web services.  Beyond HPC and HTC, emerging paradigms include Service-Oriented Architecture (SOA), virtualization leading to cloud computing, and the Internet of Things (IoT).  These paradigms, along with related concepts like clusters, grids, and peer-to-peer networks, are explored in this book.  The distinctions between these computing models, particularly distributed and cloud computing, are often debated, but this book aims to clarify their relationships.\n",
            "\n",
            "Centralized computing uses a single system with shared resources. Parallel computing uses multiple processors with shared or distributed memory. Distributed computing involves multiple autonomous computers communicating via a network. Cloud computing utilizes centralized or distributed resources, often employing parallel or distributed computing.  Concurrent computing encompasses parallel and distributed computing. Ubiquitous computing uses pervasive devices, while the Internet of Things (IoT) connects everyday objects. Internet computing covers all paradigms over the Internet.  Computational grids and data grids emerged from P2P and cluster technologies. Cloud computing arose from shifting desktop computing to service-oriented models using clusters and databases.\n",
            "\n",
            "This chapter introduces parallel and distributed systems, including grids and clouds, emphasizing resource sharing.  It discusses the design theory, technologies, and case studies of these systems, highlighting the increasing scale of computing with examples like the Tianhe-1A supercomputer.  The chapter emphasizes the growing demand for multicore processors in both High-Performance Computing (HPC) and High-Throughput Computing (HTC) systems.  It outlines key design objectives: efficiency (resource utilization and throughput), dependability (reliability and QoS), adaptation (supporting diverse job requests and data), and flexibility (running both HPC and HTC applications).  Finally, it touches upon computing trends like Moore's and Gilder's laws and the impact of commodity hardware on large-scale computing.\n",
            "\n",
            "Parallel computing has evolved from bit-level to job-level parallelism.  Early systems used bit-serial processing, transitioning to word-level processing with wider CPUs. Instruction-level parallelism (ILP) improved performance by executing multiple instructions concurrently. Data-level parallelism (DLP) uses SIMD and vector machines. Task-level parallelism (TLP) is explored with multicore processors.  Modern processors utilize all these parallelism types, though TLP implementation faces challenges.  High-performance and high-throughput computing drive advancements in various applications, including scientific simulations, business services, healthcare, internet services, and mission-critical systems, demanding transparency in resource management and execution.\n",
            "\n",
            "Distributed systems are essential across diverse fields, requiring economic computing, large-scale data handling, reliability, and scalability. Banking exemplifies this, with distributed transactions demanding consistent data replication. Challenges include software limitations, network congestion, and security risks. Modern computing paradigms, including grid, cloud, and utility computing, aim for ubiquitous, reliable, and scalable systems. These models support autonomic operations and are composable with QoS and SLAs, realizing the vision of computer utilities. Cloud computing, broader than utility computing, involves distributed applications on edge networks, presenting technological challenges in hardware, software, and management.\n",
            "\n",
            "Emerging technologies often follow a hype cycle, progressing through stages of trigger, peak of inflated expectations, disillusionment, enlightenment, and plateau of productivity.  The time to reach mainstream adoption varies, from less than two years to over ten, or even obsolescence.  In 2010, cloud computing was crossing the peak of expectations, while 3D printing and mesh network sensors were at earlier stages.  The Internet of Things (IoT) extends internet connectivity to everyday objects, creating a network of interconnected sensors and devices.  This concept, introduced in 1999, aims to connect all aspects of daily life.\n",
            "\n",
            "Gartner's 2010 Hype Cycle illustrates the maturity of emerging technologies.  It charts technologies from \"Technology Trigger\" through stages of inflated expectations, disillusionment, enlightenment, and productivity.  The cycle categorizes technologies like 3D printing, cloud computing, and mobile robots based on their expected mainstream adoption timeline (from less than 2 years to more than 10 years, or obsolete).  The graphic highlights the varying levels of maturity for different technologies and serves as a research tool, not a definitive guide.\n",
            "\n",
            "The Internet of Things (IoT) aims to connect all objects using technologies like RFID and GPS, leveraging IPv6's vast address space.  It envisions a world where humans and things interact intelligently through H2H, H2T, and T2T communication, creating a dynamic network of networks.  The IoT is still in early stages, with prototypes and cloud computing playing a crucial role in its development.  A smart Earth, facilitated by the IoT, promises improved living conditions.  Cyber-Physical Systems (CPS), closely related to the IoT, integrate computation, communication, and control to bridge the gap between the cyber and physical worlds, potentially revolutionizing our interaction with reality.\n",
            "\n",
            "Processor speed, measured in MIPS, increased significantly from 1 MIPS in 1978 to 22,000 MIPS in 2008, closely following Moore's Law.  Clock rates also rose, but plateaued around 5 GHz due to power and heat limitations.  Modern CPUs utilize multicore architectures and exploit parallelism at Instruction Level Parallelism (ILP) and Thread Level Parallelism (TLP) levels. ILP techniques include superscalar architecture and speculative execution. GPUs further explore TLP and Data Level Parallelism (DLP) with many-core architectures.  Network bandwidth has also seen substantial growth, evolving from Ethernet to Gigabit Ethernet and beyond.\n",
            "\n",
            "Multi-core CPUs and many-core GPUs handle multiple instruction threads.  Multi-core CPUs feature multiple cores, each with a private L1 cache, sharing an L2 cache. Future designs may include shared L3 caches.  Examples include Intel i7, Xeon, and AMD Opteron.  Each core can also be multi-threaded, increasing instruction-level and thread-level parallelism.  CPUs face limitations in exploiting data-level parallelism due to memory constraints, leading to the development of many-core GPUs.  x-86 architectures are increasingly used in HPC and HTC systems.  Future processors may be heterogeneous, combining CPU and GPU cores on a single chip.\n",
            "\n",
            "This passage illustrates instruction scheduling in five processor architectures: superscalar, fine-grain multithreaded, coarse-grain multithreaded, dual-core CMP, and SMT. The superscalar processor executes single-threaded instructions. Multithreaded processors interleave instructions from multiple threads, with fine-grain switching per cycle and coarse-grain switching after multiple cycles. The CMP executes threads on separate cores, while SMT schedules instructions from different threads simultaneously within a cycle. Blank slots indicate idle cycles, reflecting scheduling efficiency. The figure demonstrates how these architectures exploit instruction-level parallelism (ILP) and thread-level parallelism (TLP).\n",
            "\n",
            "GPUs, initially designed for graphics processing, have evolved into powerful parallel processors used in high-performance computing (HPC).  Unlike CPUs with a few cores, GPUs boast hundreds of cores, enabling massive parallelism by executing numerous threads concurrently.  This throughput architecture makes GPUs ideal for data-intensive calculations, not just graphics.  Modern GPUs are utilized in diverse devices, from mobile phones to supercomputers.  The CPU offloads computationally intensive tasks to the GPU, which excels at parallel floating-point operations.  NVIDIA's CUDA programming model facilitates this CPU-GPU interaction, managing data transfer and kernel execution on GPUs like GeForce and Tesla/Fermi, which are crucial components in large-scale cluster computing.\n",
            "\n",
            "The NVIDIA Fermi GPU, used in supercomputers like Tianhe-1a, features 16 streaming multiprocessors (SMs), each with up to 512 CUDA cores.  These cores, arranged in a pipelined architecture, perform parallel integer and floating-point operations.  Each SM has dedicated L1 cache, while all SMs share a unified L2 cache.  A network-on-chip connects cores and caches, while memory controllers interface with external DRAM.  SMs schedule threads in 32-thread warps, enabling massive parallel processing.  A single Fermi GPU can achieve peak performance of 82.4 Tflops.  Future GPUs may contain thousands of cores, driving the trend toward hybrid CPU-GPU architectures in exascale systems.  Challenges for such systems include power consumption, memory, concurrency, and system resilience.\n",
            "\n",
            "GPUs offer significant power efficiency advantages over CPUs, a crucial factor for future exascale systems.  CPUs consume significantly more power per instruction than GPUs.  GPUs achieve this efficiency by optimizing for throughput with explicit memory management, unlike CPUs which prioritize latency.  This difference is reflected in their performance/power ratios, where GPUs demonstrate a higher Gflops/watt per core.  The NVIDIA Fermi GPU, with its streaming multiprocessors and CUDA cores, exemplifies this architecture, focusing on parallel processing and efficient data handling.\n",
            "\n",
            "Power consumption and software development are major challenges for future parallel and distributed computing.  While GPUs show promise in closing the performance gap with CPUs, data movement remains a significant power drain.  Optimizing storage hierarchies, memory, and developing self-aware operating systems, runtime support, and locality-aware compilers are crucial.  Memory capacity has increased dramatically, but access time hasn't kept pace, exacerbating the \"memory wall\" problem.  Disk storage has also seen massive growth, and the emergence of SSDs further impacts high-performance computing.  The widening gap between processor speed and memory access time poses a significant performance bottleneck.\n",
            "\n",
            "Flash and SSD drives offer significant speed improvements over traditional hard disks.  While SSDs are currently too expensive for widespread replacement of disk arrays, they represent the future of storage technology.  System-area interconnects, such as Ethernet switches and LANs, connect nodes in small clusters.  Storage area networks (SANs) link servers to network storage, while network-attached storage (NAS) connects clients directly to disk arrays.  Small clusters without shared distributed storage can utilize a multiport Gigabit Ethernet switch.  Memory and disk technologies have seen dramatic improvements in capacity over the past decades.\n",
            "\n",
            "Ethernet bandwidth has rapidly increased, exceeding Moore's Law, enabling larger distributed systems.  High-bandwidth networking facilitates massively distributed systems, with InfiniBand and Ethernet being key interconnect choices in High-Performance Computing.  Data centers commonly use Gigabit Ethernet. Virtual machines (VMs) address limitations of traditional single OS systems, improving resource utilization, application flexibility, and security.  Cloud computing relies on virtualization of processors, memory, and I/O.  VMs, virtual storage, and virtual networking, along with their associated middleware, are crucial components of this virtualized environment.  Three network types connect servers, clients, and storage: LANs, SANs, and NAS.\n",
            "\n",
            "Virtual machines (VMs) provide a way to run multiple operating systems on a single physical machine.  A virtual machine monitor (VMM), also known as a hypervisor, manages the virtual resources allocated to each VM.  There are different VM architectures, including native VMs (bare-metal) where the hypervisor interacts directly with the hardware, hosted VMs where the VMM runs on top of a host OS, and dual-mode VMs where parts of the VMM run at both user and supervisor levels. VMs offer hardware independence, allowing applications bundled with their OS to be ported to different hardware platforms.  The VMM provides the VM abstraction to the guest OS, enabling standard operating systems to run as if they were on physical hardware.\n",
            "\n",
            "Virtual machines (VMs) offer several key operations: multiplexing (sharing hardware), suspension (saving to storage), provisioning/resumption (starting on new hardware), and migration (moving between hardware).  These capabilities allow VMs to be deployed on any available platform, improving distributed application portability and server utilization.  By consolidating multiple servers onto single hardware platforms as VMs, server sprawl is reduced and efficiency increases.  VMware estimates utilization can rise from 5-15% to 60-80%.  Virtual infrastructure dynamically maps physical compute, storage, and networking resources to applications within VMs, separating hardware and software. This leads to cost reduction, increased efficiency, and responsiveness, exemplified by server consolidation.\n",
            "\n",
            "Cloud data centers utilize commodity hardware like x86 processors and Gigabit Ethernet, prioritizing cost-effectiveness over raw speed. Server growth has increased significantly, with millions in use. Operational costs, particularly utilities, now exceed hardware expenses. A large portion of data center expenditures goes towards cooling and power infrastructure, not just IT equipment. The low-cost design philosophy emphasizes efficiency, with a focus on storage and energy savings. Network choices reflect this, favoring budget-conscious solutions over high-end, expensive options.\n",
            "\n",
            "Cloud computing leverages commodity hardware and software, relying on virtualization and multi-core chips, utility and grid computing, SOA, Web 2.0, and autonomic computing.  It addresses the growing challenge of managing and analyzing massive datasets (\"data deluge\") by providing scalable tools for data capture, creation, and analysis. This convergence of cloud technology, e-science, and multi-core computing is driving innovation in various scientific disciplines, enabling new insights through data-intensive technologies like MapReduce.  Cloud computing offers on-demand services at different levels, transforming how we interact with information and facilitating data-driven scientific discovery.\n",
            "\n",
            "Distributed and cloud computing systems connect numerous autonomous nodes via SANs, LANs, or WANs, forming hierarchical structures.  These massive, scalable systems range from hundreds to millions of nodes, achieving web-scale connectivity.  Four main classes exist: clusters, peer-to-peer networks, computing grids, and cloud platforms.  Clusters offer high-performance computing, while P2P networks facilitate file sharing and social networking.  Grids enable distributed supercomputing, and cloud platforms provide utility and outsourced computing.  Each class differs in architecture, control, resource management, and typical applications, exemplified by systems like Google Search, BitTorrent, TeraGrid, and AWS.\n",
            "\n",
            "Clusters, popular in supercomputing, form the foundation for grids and clouds.  They consist of interconnected, independent computers working together.  Cluster architecture typically involves a high-bandwidth, low-latency interconnection network (SAN or LAN) connecting server nodes.  Larger clusters can be built hierarchically using multiple levels of switches.  A VPN gateway connects the cluster to the internet, providing a single IP address.  Most clusters are loosely coupled, with each node managing its own resources and OS, resulting in multiple system images.  Shared I/O devices and disk arrays are common.\n",
            "\n",
            "An ideal cluster creates a single-system image (SSI), making it appear as one machine to the user, though this is difficult.  Most high-performance computing (HPC) clusters, often called MPPs, consist of interconnected computer nodes running Linux, using high-bandwidth networks and special communication software like PVM or MPI.  Middleware is crucial for SSI and high availability.  Many clusters are loosely coupled, though virtualization allows for dynamic virtual clusters.  A true cluster-wide OS is lacking, so middleware facilitates cooperative computing and high performance.  Grid computing builds upon the internet and web, enabling interaction between applications on distant computers, playing a significant role in the growth of the IT economy.\n",
            "\n",
            "A computational grid links diverse computing resources (workstations, servers, clusters, supercomputers) across networks (LAN, WAN, Internet) into a unified platform.  It can span regional, national, or global scales, often involving specialized instruments like radio telescopes.  Users access the grid through various devices, viewing it as a single, integrated resource pool.  Grids support virtual organizations and offer computing, communication, content, and transaction services.  Key design considerations for clusters within grids include high availability, fault tolerance, single system image, efficient communication, job management, load balancing, and scalability.  These are addressed through techniques like redundancy, middleware, fast messaging, and distributed execution environments.\n",
            "\n",
            "Grid computing has evolved rapidly, mirroring the growth of the internet.  Two primary grid categories exist: computational/data grids and peer-to-peer (P2P) grids.  Computational grids, often national initiatives like TeraGrid, ChinaGrid, and EGEE, focus on distributed supercomputing and resource sharing among organizations.  P2P grids, exemplified by systems like JXTA and SETI@home, leverage user-contributed resources.  While computational grids face challenges like middleware bugs and restricted access, P2P grids struggle with resource reliability and limited application scope.  Both grid types require specialized software, middleware, and network protocols.\n",
            "\n",
            "Peer-to-peer (P2P) systems offer a distributed network model where each node acts as both client and server, sharing resources.  Unlike client-server architectures, P2P networks lack central coordination or a master-slave relationship.  Peers join and leave the network freely, resulting in a self-organizing system with distributed control.  The physical network is an ad-hoc formation of participating internet-connected client machines using standard protocols.  Above this physical layer, an overlay network is formed at the logical level.  This overlay network connects peers based on communication and file-sharing needs, creating virtual links atop the physical IP network.  The dynamic nature of peer participation means both physical and overlay networks constantly change.\n",
            "\n",
            "Peer-to-peer (P2P) networks logically connect machines via a virtual mapping.  New peers are added as nodes, and leaving peers are removed.  Overlay networks, either unstructured (random connections) or structured (defined topology), dictate connectivity. Unstructured networks often use flooding for queries, while structured networks employ routing mechanisms.  P2P applications include file sharing (e.g., BitTorrent), collaboration (e.g., Skype), and distributed computing (e.g., SETI@home).  Challenges include hardware, software, and network heterogeneity, as well as security, trust, and standardization issues.\n",
            "\n",
            "Peer-to-peer (P2P) networks offer scalability and distributed resource sharing but suffer from management, security, and reliability issues.  While robust against single points of failure, data loss can occur.  P2P networks are suitable for small-scale, low-security applications. Cloud computing, driven by data explosion, shifts computation to data centers.  It provides on-demand, virtualized resources, supporting scalable workloads and self-recovery.  Cloud platforms offer cost-effectiveness through virtualization, aiming to replace desktop computing with service-oriented models based on large data centers.\n",
            "\n",
            "Cloud computing addresses limitations of traditional distributed computing systems like high maintenance costs and poor resource utilization.  It offers on-demand computing through three service models: Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). IaaS provides users with access to virtualized infrastructure like servers and storage. PaaS allows users to deploy applications onto a virtualized platform including middleware and development tools. SaaS delivers browser-initiated application software, eliminating upfront investment in servers or software licensing for the customer.  These models form a cloud landscape where users access resources and services over the internet.\n",
            "\n",
            "Cloud computing offers private, public, managed, and hybrid deployment models, each with distinct security implications and shared responsibilities among providers, consumers, and software vendors.  Adopting the cloud offers benefits like location advantages, peak-load sharing, separation of maintenance and development, cost reduction, streamlined programming, service discovery, and content distribution. However, it also raises concerns about privacy, security, copyright, reliability, service agreements, and pricing.  Cloud computing utilizes various software environments.  Three key service models are IaaS (Infrastructure as a Service), providing data centers and VM hosting; PaaS (Platform as a Service), offering programming environments for application development and deployment; and SaaS (Software as a Service), delivering software applications from the cloud to desktops.\n",
            "\n",
            "Distributed systems like web services and grids utilize a layered architecture.  This architecture builds upon the traditional seven OSI layers, but operates at the entity level (service, object) rather than the bit level.  Key components include entity interfaces (WSDL, Java methods, CORBA IDL), high-level communication systems (SOAP, RMI, IIOP) often built on message-oriented middleware (e.g., JMS), and higher-level services for registries, metadata, and management.  These layers handle features like fault tolerance (WSRM), security, and message patterns (RPC).  The architecture allows for specialized distributed computing features, effectively rebuilding the top four OSI layers for entities.\n",
            "\n",
            "Distributed systems utilize either a distributed object model (e.g., CORBA, Java RMI) or a service-oriented model (e.g., web services, REST).  Distributed objects offer shared memory advantages but distributed systems provide better performance and cleaner separation of functions. Web services, using SOAP, aim for complete service specification and a universal distributed operating system, but face implementation challenges. REST prioritizes simplicity, delegating complex tasks to application software.  While REST is suitable for dynamic environments, web services concepts are valuable for mature systems.  Distributed entities, like sensors, grids, and clouds, can be composed into applications using techniques like RPCs and RMIs.\n",
            "\n",
            "Service-oriented architecture (SOA) has evolved to encompass various distributed systems, including grids, clouds, and their combinations.  Sensors collect raw data, interacting with diverse computing resources like grids, databases, and cloud platforms (compute, storage, filter, discovery). Filter services refine this data, forming filter clouds. SOA's goal is to extract valuable information from massive datasets.  Subsequent chapters will delve into specific cloud types and distributed system paradigms like grids, P2P networks, and the Internet of Things (IoT).\n",
            "\n",
            "Distributed systems process raw data through compute and storage clouds, culminating in user-accessible portals. Grid systems use static resources, while clouds emphasize elastic ones, with potential for hybrid models like \"clouds of grids.\" Distributed operating systems enhance resource sharing, moving from network OS with file sharing to middleware solutions and ultimately, fully integrated distributed OS for greater transparency and efficiency. These systems aim to transform raw data into useful information and knowledge, facilitating intelligent decisions. Various technologies, including web services and workflows, support these distributed architectures.\n",
            "\n",
            "Amoeba, DCE, and MOSIX2 are research prototypes for distributed operating systems.  Amoeba, developed at Free University, uses a microkernel approach. DCE, pushed by the Open Software Foundation, is middleware-based and extends existing OSs like UNIX. MOSIX2, from Hebrew University, runs on Linux, providing a single-system image and supporting process migration.  All three systems aimed to distribute resource management.  Amoeba is microkernel-based, DCE is middleware, and MOSIX2 uses a virtualization layer within Linux.  They differ in communication mechanisms and support for features like security and process migration.  None achieved widespread commercial success.\n",
            "\n",
            "MOSIX facilitates resource sharing among cluster owners, extending grid capabilities.  MOSIX2 is being explored for diverse cluster management, including Linux, GPU, grids, and clouds.  A transparent computing infrastructure separates user data, applications, OS, and hardware.  Data remains user-owned, independent of applications.  Standard interfaces enable OS and hardware selection.  Cloud applications as SaaS further decouple data from specific programs, allowing users to switch services.  Four distributed programming models will be explored, including MPI for message-passing systems and Google's MapReduce/BigTable.\n",
            "\n",
            "Distributed computing utilizes various models and tools.  Message-Passing Interface (MPI) is a primary standard for parallel programming on distributed systems, using libraries callable from C or FORTRAN. MapReduce is a web programming model for scalable data processing on large clusters, employing Map and Reduce functions. Hadoop, a software platform, enables users to write and run applications over vast distributed datasets, offering a scalable and economical solution with an open-source MapReduce version.  These tools and models address the challenges of effectively using resources in internet clouds and data centers.\n",
            "\n",
            "Grid computing leverages distributed resources for large-scale applications.  OGSA provides a standard framework for grid services, exemplified by Genesis II, offering distributed execution, PKI security, and trust management. Globus, a middleware toolkit, implements OGSA standards for resource management and security, supporting multi-site authentication.  IBM has extended Globus for business applications.  Key considerations for distributed systems include performance, security, and energy efficiency, encompassing scalability, availability, programming models, and security across clusters, grids, P2P networks, and clouds.\n",
            "\n",
            "Performance in distributed systems is measured by metrics like throughput (MIPS, TFLOPS, TPS), response time, and latency.  Scalability has several dimensions: size (increasing hardware resources), software (OS and compiler upgrades), application (matching problem size with machine size), and technology (adapting to new components).  Size scalability is exemplified by increases in processor counts. Software scalability involves OS and library upgrades. Application scalability focuses on increasing problem size. Technology scalability considers time (generational changes), space (packaging and energy), and heterogeneity (multi-vendor components).\n",
            "\n",
            "Scalable performance in distributed systems is measured by increasing processors, memory, disk capacity, or I/O channels.  OS image count reflects independent OS instances within a system. SMP systems, with a single OS image, scale less than NUMA machines, which can run multiple OSs. Clusters, composed of loosely coupled nodes (often SMP servers), offer even greater scalability. Clouds, virtualized clusters, also scale to thousands of VMs.  Grids, encompassing diverse systems like clusters and mainframes, have fewer OS images than processors.  P2P networks, comprised of independent peer nodes, exhibit the highest scalability, reaching millions of OS images.  Generally, the number of processors in a system significantly exceeds the number of OS images.\n",
            "\n",
            "Amdahl's Law defines the speedup limit of a program parallelized for *n* processors, based on the fraction *α* of code that must be executed sequentially.  The maximum speedup is 1/*α*, regardless of *n*.  Even with many processors, speedup is limited by the sequential bottleneck.  Fixed-workload speedup assumes a constant problem size, often resulting in low efficiency with large clusters as many processors idle.  Gustafson's Law addresses this by scaling the problem size with the number of processors, increasing the workload for the parallelizable portion to maintain efficiency in large clusters.\n",
            "\n",
            "Gustafson's Law defines scaled-workload speedup, showing that speedup increases linearly with the number of processors when problem size scales with processor count.  Efficiency in this scenario is calculated considering the fraction of parallelizable work.  Amdahl's Law applies to fixed workloads, while Gustafson's Law is for scaled problems.  System availability, crucial for distributed systems, is defined as MTTF/(MTTF+MTTR).  High availability requires long MTTF and short MTTR.  Single points of failure must be avoided.  Availability generally decreases with system size due to increased failure probability.  Clusters, clouds, and grids exhibit decreasing availability with scale, while P2P networks, despite size, have low individual node availability.\n",
            "\n",
            "Network threats, including viruses and attacks, pose significant risks to distributed systems like clusters, grids, P2P networks, and clouds, potentially causing financial losses and data breaches.  These threats can compromise confidentiality, integrity, and availability.  Different service models (SaaS, PaaS, IaaS) distribute security responsibilities between providers and users.  SaaS providers handle most security, while IaaS users are responsible for most, except availability. PaaS falls in between, with providers ensuring integrity and availability, and users managing confidentiality.  Protecting these systems is crucial for user trust and adoption, as intrusions can damage resources and hinder the growth of public-resource computing.\n",
            "\n",
            "P2P copyright infringement is primarily driven by collusion between paid and unpaid users.  A proactive content poisoning scheme using identity-based signatures and timestamps can deter this activity without impacting legitimate users.  Network defense has evolved through three generations: prevention, detection, and intelligent response.  A secure infrastructure is crucial for web and cloud services, requiring trust negotiation and reputation aggregation.  Various attacks, including eavesdropping, masquerade, and denial of service, threaten confidentiality, integrity, and availability, leading to data loss and system disruption.\n",
            "\n",
            "Cloud security involves protecting against various attacks, with responsibilities divided between providers and users based on service models. IaaS users handle confidentiality, while providers ensure data integrity. PaaS and SaaS share responsibility. Energy efficiency in distributed computing is crucial due to high energy costs and environmental impact. Large-scale systems like Earth Simulator consume significant power, leading to substantial expenses and cooling issues. A significant portion of servers are unused, resulting in billions of dollars in wasted energy and substantial carbon emissions. Identifying and deactivating these idle servers is vital for cost reduction and environmental sustainability.\n",
            "\n",
            "Energy consumption in active distributed systems can be reduced by applying techniques at four layers: application, middleware, resource, and network.  The application layer focuses on designing energy-aware applications without sacrificing performance, moving beyond simply prioritizing speed and quality.  The middleware layer provides services like resource brokering, task scheduling, and communication, which can be optimized for energy efficiency. The resource layer comprises the physical servers, laptops, and other hardware, while the network layer includes routers, switches, and connections.  Optimizing energy usage at each layer contributes to overall system efficiency.\n",
            "\n",
            "Distributed computing systems consist of application, middleware, resource, and network layers.  The middleware layer manages resources and applies energy-efficient task scheduling, balancing makespan and energy consumption. The resource layer comprises computing nodes and storage, utilizing techniques like Dynamic Power Management (DPM) and Dynamic Voltage-Frequency Scaling (DVFS) for power efficiency.  The network layer handles routing and packet transfer, facing challenges in balancing energy consumption and performance.  Developing comprehensive network models and energy-efficient routing algorithms are crucial. Data centers are increasingly vital for information storage, processing, and service provision, becoming core infrastructure.  Performance is tied to energy consumption, influenced by computation and storage demands.\n",
            "\n",
            "Traditional data centers face challenges like high costs, complex management, and energy consumption.  Dynamic Voltage Frequency Scaling (DVFS) offers a solution by exploiting idle time.  It reduces voltage and frequency, thus lowering energy use, based on the relationship between energy, voltage, and frequency in CMOS circuits (E = Ceff fv2t).  DVFS reduces energy consumption during workload slack time by switching to lower power modes with minimal latency.  This contrasts with traditional methods that maintain constant frequency/voltage.  Storage units, contributing significantly to data center energy use (around 27%), must be considered in power management strategies alongside computing nodes.\n",
            "\n",
            "This section provides a bibliographic overview of parallel and distributed computing, highlighting key research areas and resources. It covers historical and recent developments in parallel computing, distributed systems, cluster, grid, and cloud computing, P2P networks, and multicore/many-core processors. Data centers, cloud computing business models, and virtualization techniques are discussed, along with challenges in data-intensive computing and security. The section also lists relevant conferences, magazines, and journals from IEEE, ACM, and other organizations, offering a guide to current research and publications in the field.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(new_text)):\n",
        "  prompt =f\"\"\"\n",
        "  Your task is to act as a Text Summariser.\n",
        "  I'll give you text from  pages of a book from beginning to end.\n",
        "  And your job is to summarise text from these pages in less than 100 words.\n",
        "  Don't be conversational. I need a plain 100 word answer.\n",
        "  Text is shared below, delimited with triple backticks:\n",
        "  ```{text[i]}```\n",
        "  \"\"\"\n",
        "  try:\n",
        "    response = get_completion(prompt)\n",
        "  except:\n",
        "    response = get_completion(prompt)\n",
        "  print(response)\n",
        "  summary= summary+' ' +response +'\\n\\n'\n",
        "  # result.append(response)\n",
        "  time.sleep(19)  #You can query the model only 3 times in a minute for free, so we need to put some delay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "uP7gZAxliIP1"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/summary.txt',\n",
        "          'w') as out:\n",
        "  out.write(summary)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
